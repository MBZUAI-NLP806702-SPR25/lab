{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training with Deepspeed\n",
    "\n",
    "DeepSpeed is an open-source deep learning optimization library developed by Microsoft, designed to enhance the efficiency and scalability of training large models. It offers features like the Zero Redundancy Optimizer (ZeRO) for efficient memory usage, support for mixed precision training, and seamless integration with PyTorch/Huggingface. DeepSpeed is particularly beneficial when training models with billions of parameters, enabling faster training times and reduced resource consumption.\n",
    "\n",
    "## ZeRO Optimization:\n",
    "\n",
    "DeepSpeed’s ZeRO (Zero Redundancy Optimizer) strategy removes memory redundancy by partitioning key model states (parameters, gradients, and optimizer states) across GPUs. Here’s a step‐by‐step breakdown of its three primary stages:\n",
    "\n",
    "![ZeRO](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-26_at_3.17.43_PM_3oyU7Qb.png)\n",
    "\n",
    "1. Stage 1 – Optimizer State Partitioning:\n",
    "In this stage, the optimizer’s internal states (for example, moment estimates in Adam) are split among the GPUs. Instead of every GPU storing a complete copy of these states, each holds only a fraction. This immediately cuts down memory usage without affecting the forward or backward computations.\n",
    "\n",
    "\n",
    "2. Stage 2 – Gradient Partitioning:\n",
    "Building on stage 1, ZeRO Stage 2 partitions not only the optimizer states but also the gradients. During the backward pass, each GPU computes and stores only its portion of the gradients. Later, a communication step (such as an all-reduce operation) ensures that the required information is combined for the optimizer update—all while keeping the memory footprint much lower than storing full gradients on every device.\n",
    "\n",
    "\n",
    "3. Stage 3 – Parameter Partitioning:\n",
    "The most aggressive stage, Stage 3, partitions the model’s parameters themselves. Now, each GPU holds only a slice of the entire model’s weights, along with its corresponding gradients and optimizer states. When a forward pass is performed, the necessary parameters are gathered on the fly. This stage enables training extremely large models that would not fit in the memory of a single GPU, though it may introduce additional communication overhead.\n",
    "In practice, during training the forward pass uses the available partitions (or gathers full parameters as needed in Stage 3), the backward pass computes local gradients that are later synchronized, and the optimizer updates are applied based only on the locally stored states. This stepwise, partitioned approach is critical when working with models of billions (or even trillions) of parameters, as it enables efficient distributed training by dramatically reducing memory consumption on each device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Deepspeed and transformers\n",
    "The transformers library has native support for deepspeed in its Trainers using the accelerate library which includes deepspeed and other parallelism strategies (like Data parallelism, etc. see docs for more details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Installing deepspeed\n",
    "\n",
    "```bash\n",
    "DS_BUILD_CPU_ADAM=1 pip install deepspeed==0.15.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with and without Deepspeed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary libraries\n",
    "\n",
    "```bash\n",
    "pip install argparse transformers datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without deepspeed/distributed trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Deepspeed\n",
    "Ensuring only one device/GPU gets used, this model (gpt2) can actually run on a single devices\n",
    "\n",
    "```bash\n",
    "export USE_DEEPSPEED=false && export CUDA_VISIBLE_DEVICES=0 && accelerate launch ift.py --model_name gpt2\n",
    "```\n",
    "\n",
    "To check GPU usage:\n",
    "\n",
    "```bash\n",
    "nvidia-smi \n",
    "```\n",
    "Example Output:\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |\n",
    "| N/A   42C    P0   282W / 300W |   9591MiB / 32768MiB |    100%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |\n",
    "| N/A   34C    P0    44W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   2  Tesla V100-SXM2...  On   | 00000000:3A:00.0 Off |                    0 |\n",
    "| N/A   30C    P0    45W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   3  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\n",
    "| N/A   33C    P0    39W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   4  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
    "| N/A   32C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   5  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
    "| N/A   34C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   6  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |\n",
    "| N/A   32C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   7  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |\n",
    "| N/A   33C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A     25677      C   ...da3/envs/madar/bin/python     9588MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Deepspeed\n",
    "\n",
    "Let's check how many GPUs on the machine\n",
    "\n",
    "```bash\n",
    "nvidia-smi  -L | wc -l\n",
    "```\n",
    "\n",
    "Set deepspeed config\n",
    "\n",
    "```bash\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "```bash\n",
    "export USE_DEEPSPEED=true && export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 && accelerate launch --config_file ./default_config.yaml ift.py --model_name gpt2 \n",
    "```\n",
    "\n",
    "To check GPU usage:\n",
    "\n",
    "```bash\n",
    "watch -n0.1 nvidia-smi\n",
    "```\n",
    "Example Output:\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |\n",
    "| N/A   35C    P0    70W / 300W |   6731MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |\n",
    "| N/A   37C    P0    72W / 300W |   6691MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   2  Tesla V100-SXM2...  On   | 00000000:3A:00.0 Off |                    0 |\n",
    "| N/A   34C    P0    72W / 300W |   6595MiB / 32768MiB |      1%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   3  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\n",
    "| N/A   36C    P0    66W / 300W |   6507MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   4  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
    "| N/A   36C    P0    69W / 300W |   6651MiB / 32768MiB |      1%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   5  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
    "| N/A   38C    P0    71W / 300W |   6779MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   6  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |\n",
    "| N/A   35C    P0    69W / 300W |   6587MiB / 32768MiB |      1%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   7  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |\n",
    "| N/A   36C    P0    70W / 300W |   6635MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A     34717      C   ...da3/envs/madar/bin/python     6728MiB |\n",
    "|    1   N/A  N/A     34718      C   ...da3/envs/madar/bin/python     6688MiB |\n",
    "|    2   N/A  N/A     34719      C   ...da3/envs/madar/bin/python     6592MiB |\n",
    "|    3   N/A  N/A     34720      C   ...da3/envs/madar/bin/python     6504MiB |\n",
    "|    4   N/A  N/A     34721      C   ...da3/envs/madar/bin/python     6648MiB |\n",
    "|    5   N/A  N/A     34722      C   ...da3/envs/madar/bin/python     6776MiB |\n",
    "|    6   N/A  N/A     34723      C   ...da3/envs/madar/bin/python     6584MiB |\n",
    "|    7   N/A  N/A     34724      C   ...da3/envs/madar/bin/python     6632MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Questions:\n",
    "\n",
    "1. Does it make sense to use Deepspeed here?\n",
    "2. Which setting has a faster training time? Why?\n",
    "3. Try it with a bigger model if you have more than one device. First, run it without deepspeed. Keep choosing bigger and bigger models until you run into memory issues, then try it with Deepspeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Example wtih Pytorch\n",
    "\n",
    "We saw how to do it with transformers automatically, what if we have a training loop from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop (PyTorch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./sft_train_manual.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./sft_train_manual.py\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "import deepspeed\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Manual SFT Training with optional DeepSpeed\")\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, required=True,\n",
    "                        help=\"Hugging Face model name or path (e.g. 'distilgpt2')\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./sft_manual\")\n",
    "    parser.add_argument(\"--use_deepspeed\", action=\"store_true\", help=\"Enable DeepSpeed\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=512)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--micro_batch\", type=int, default=1)\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
    "    parser = deepspeed.add_config_arguments(parser)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def preprocess_function(example, tokenizer, max_length=512):\n",
    "    # Assume each example has 'instruction', optional 'input', and 'output'\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    user_input = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    \n",
    "    # Build a prompt string\n",
    "    if user_input:\n",
    "        prompt = f\"Instruction: {instruction}\\nInput: {user_input}\\nResponse:\"\n",
    "    else:\n",
    "        prompt = f\"Instruction: {instruction}\\nResponse:\"\n",
    "    full_text = prompt + \" \" + output\n",
    "    \n",
    "    # Tokenize full text to fixed length with padding\n",
    "    tokenized = tokenizer(full_text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "    # Tokenize prompt (without padding) to know its token length\n",
    "    prompt_tokens = tokenizer(prompt, truncation=True, add_special_tokens=False)[\"input_ids\"]\n",
    "    prompt_len = len(prompt_tokens)\n",
    "    \n",
    "    # Create labels as a copy of input_ids\n",
    "    labels = copy.deepcopy(tokenized[\"input_ids\"])\n",
    "    # For next-token prediction the model automatically shifts labels.\n",
    "    # Here, we mask out the prompt portion (positions 1 up to prompt_len) by setting them to -100.\n",
    "    for i in range(1, min(prompt_len, len(labels))):\n",
    "        labels[i] = -100\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path).to(device)\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load a sample instruction dataset (here, a cleaned Alpaca dataset)\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    # Select 10k examples if dataset is larger\n",
    "    if len(dataset) > 10000:\n",
    "        dataset = dataset.shuffle(seed=42).select(range(10000))\n",
    "    \n",
    "    # Preprocess each example (build full text and mask prompt tokens)\n",
    "    dataset = dataset.map(lambda x: preprocess_function(x, tokenizer, max_length=args.max_length))\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Optionally initialize DeepSpeed\n",
    "    if args.use_deepspeed:\n",
    "        # A simple DeepSpeed config: enable fp16 training and set batch size\n",
    "        ds_config = {\n",
    "            \"train_batch_size\": args.batch_size,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"fp16\": {\n",
    "                \"enabled\": True\n",
    "            },\n",
    "            \"zero_optimization\": {\n",
    "                \"stage\": 3,\n",
    "            },\n",
    "            \"zero_allow_untested_optimizer\": True,\n",
    "        }\n",
    "        model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "            args=args,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            config=ds_config,\n",
    "            model_parameters=model.parameters()\n",
    "        )\n",
    "        print(\"Training with DeepSpeed enabled.\")\n",
    "    else:\n",
    "        model_engine = model\n",
    "        print(\"Training without DeepSpeed.\")\n",
    "    \n",
    "    model_engine.train()\n",
    "    for epoch in range(args.epochs):\n",
    "        total_loss = 0.0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # Move batch tensors to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_engine(**batch)\n",
    "            loss = outputs.loss\n",
    "            if args.use_deepspeed:\n",
    "                model_engine.backward(loss)\n",
    "                model_engine.step()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1} Step {step} Loss {loss.item():.4f}\")\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    if args.use_deepspeed:\n",
    "        model_engine.save_checkpoint(args.output_dir)\n",
    "    else:\n",
    "        model.save_pretrained(args.output_dir)\n",
    "    print(\"Training complete. Model saved to\", args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python sft_train_manual.py --model_name_or_path gpt2\n",
    "deepspeed --num_gpus=8 sft_train_manual.py --model_name_or_path gpt2 --use_deepspeed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "1. Can we train on more than one node?\n",
    "2. What is the difference between micro_batch_size and batch_size? Does it matter?\n",
    "3. If I want to do something fancy, do I need to implement everything from scratch?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madar",
   "language": "python",
   "name": "madar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
