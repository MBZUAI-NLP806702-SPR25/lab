{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers scikit-learn numpy matplotlib accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MODEL_TEACHER = \"google-bert/bert-base-uncased\"\n",
    "MODEL_STUDENT = \"prajjwal1/bert-small\"\n",
    "DATASET_NAME = \"SetFit/20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        dataset_name: str,\n",
    "        max_length: int,\n",
    "        batch_size: int,\n",
    "        split_min_max=(3, 256),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.split_min_max = split_min_max\n",
    "        self.dataset_name = dataset_name\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        dataset = load_dataset(self.dataset_name)\n",
    "        # filter dataset to include text > 3 tokens and < 256 tokens\n",
    "        self.dataset = dataset.filter(\n",
    "            lambda x: len(x[\"text\"].split()) > self.split_min_max[0]\n",
    "            and len(x[\"text\"].split()) < self.split_min_max[1]\n",
    "        )\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "\n",
    "    def tokenize(self, example):\n",
    "        return self.tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if self.train_data is not None and self.test_data is not None:\n",
    "            return\n",
    "        self.train_data = self.dataset[\"train\"]\n",
    "        self.test_data = self.dataset[\"test\"]\n",
    "        \n",
    "        self.train_data = self.train_data.map(self.tokenize)\n",
    "        self.test_data = self.test_data.map(self.tokenize)\n",
    "\n",
    "        self.train_data.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "        )\n",
    "        self.test_data.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=DataCollatorWithPadding(self.tokenizer),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=DataCollatorWithPadding(self.tokenizer),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitTrainingLogic(pl.LightningModule):\n",
    "    def __init__(self, student, teacher=None, kd_mode=None):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.kd_mode = kd_mode\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.student(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def kl_loss(self, student_logits, teacher_logits):\n",
    "        if self.kd_mode == \"forward\":\n",
    "            teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
    "            student_log_probs = F.log_softmax(student_logits, dim=-1)\n",
    "            return F.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\")\n",
    "        else:  # Reverse KL\n",
    "            student_probs = F.softmax(student_logits, dim=-1)\n",
    "            teacher_log_probs = F.log_softmax(teacher_logits, dim=-1)\n",
    "            return F.kl_div(teacher_log_probs, student_probs, reduction=\"batchmean\")\n",
    "    \n",
    "    def normal_training_step(self, batch, batch_idx):\n",
    "        student_out = self.student(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        return student_out.logits, student_out.loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if self.kd_mode is None:\n",
    "            return self.normal_training_step(batch, batch_idx)[1]\n",
    "        else:\n",
    "            raise NotImplementedError(\"KL-based training not implemented yet\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        preds = self(batch[\"input_ids\"], batch[\"attention_mask\"]).logits\n",
    "        acc = (preds.argmax(-1) == batch[\"labels\"]).float().mean()\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-5)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model = AutoModelForSequenceClassification.from_pretrained(MODEL_TEACHER, num_labels=20)\n",
    "small_model = AutoModelForSequenceClassification.from_pretrained(MODEL_STUDENT, num_labels=20)\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(MODEL_TEACHER)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(MODEL_STUDENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_dm_large = LitDataModule(\n",
    "    tokenizer=large_tokenizer,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    max_length=256,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 4\n",
    "\n",
    "\n",
    "lit_model_teacher = LitTrainingLogic(\n",
    "    student=large_model,\n",
    "    kl_mode=None\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator=\"auto\",\n",
    "    log_every_n_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lit_model_teacher, lit_dm_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_dm_large = LitDataModule(\n",
    "    tokenizer=large_tokenizer,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    max_length=256,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(lit_model_teacher, lit_dm_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model.push_to_hub(\"bert-base-uncased-20-newsgroup\")\n",
    "large_tokenizer.push_to_hub(\"bert-base-uncased-20-newsgroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
