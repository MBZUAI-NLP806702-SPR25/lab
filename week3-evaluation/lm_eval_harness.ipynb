{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Evaluation Harness\n",
    "\n",
    "Tutorial on how to setup and use the lm-evaluation-harness to evaluate LMs on a variety of pre-existing tasks as well as implementing custom tasks from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the lm-evaluation-harness. Run the following commands on Terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
    "cd lm-evaluation-harness\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lm-evaluation-harness (LEH) automates the evaluation processes for many tasks across many model types and architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with some basic evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEH primarily works through commandline to facilitate quick evaluation and prototyping without having to ever run python manually. For example, if we want to test the famous SST2 (sentiment analysis) benchmark on the newly released distilled DeepSeek R1 1.5B model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run both generative and discriminative tasks, MMLU is discriminative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "    --tasks sst2 \\\n",
    "    --device auto \\\n",
    "    --batch_size 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced options like more nuanced parallelism, using with OpenAI API, etc. check the documentation [here](https://github.com/EleutherAI/lm-evaluation-harness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```\n",
    "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
    "|-----|------:|------|-----:|------|---|-----:|---|-----:|\n",
    "|sst2 |      1|none  |     0|acc   |↑  |0.5608|±  |0.0168|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking what tasks are available and how to implement a custom task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madar",
   "language": "python",
   "name": "madar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
