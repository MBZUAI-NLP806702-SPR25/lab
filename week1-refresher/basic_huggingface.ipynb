{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a  model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer vocabulary size: 50257\n",
      "\n",
      "Model architecture:\n",
      "GPT2LMHeadModel\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"  #  Small gpt-2 model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"\\nTokenizer vocabulary size:\", len(tokenizer))\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized output:\n",
      "{'input_ids': tensor([[15496,    11,   314,   716,  4673,   399, 19930,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Decoded text: Hello, I am learning NLP!\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Hello, I am learning NLP!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized output:\")\n",
    "print(tokens)\n",
    "\n",
    "# Decode tokens back to text\n",
    "decoded_text = tokenizer.decode(tokens[\"input_ids\"][0])\n",
    "print(\"\\nDecoded text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at individual tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual tokens:\n",
      "ID: 15496, Token: Hello\n",
      "ID: 11, Token: ,\n",
      "ID: 314, Token:  I\n",
      "ID: 716, Token:  am\n",
      "ID: 4673, Token:  learning\n",
      "ID: 399, Token:  N\n",
      "ID: 19930, Token: LP\n",
      "ID: 0, Token: !\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokens[\"input_ids\"][0].tolist()\n",
    "print(\"\\nIndividual tokens:\")\n",
    "for token_id in token_ids:\n",
    "    print(f\"ID: {token_id}, Token: {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass and generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output logits shape: torch.Size([1, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    print(\"\\nOutput logits shape:\", outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The future of AI is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "The future of AI is not yet clear.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Greedy decoding\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"\\nGenerated text:\\n{generated_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will dive deep into more features in huggingface (training) in the following weeks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sushi_biryani",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
