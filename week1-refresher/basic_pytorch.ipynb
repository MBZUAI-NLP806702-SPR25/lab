{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = torch.tensor([5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: tensor(10)\n",
      "Mean: tensor(2.5000)\n",
      "Element-wise addition: tensor([ 6,  8, 10, 12])\n",
      "Element-wise multiplication: tensor([ 5, 12, 21, 32])\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "print(\"Sum:\", torch.sum(a))  # Sum all elements\n",
    "print(\"Mean:\", torch.mean(a.float()))  # Mean (convert to float for division)\n",
    "print(\"Element-wise addition:\", a + b)\n",
    "print(\"Element-wise multiplication:\", a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication: tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "Matrix multiplication (similar): tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "Hadamard product (element-wise multiplication): tensor([[ 5, 12],\n",
      "        [21, 32]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix operations\n",
    "matrix1 = torch.tensor([[1, 2], [3, 4]])\n",
    "matrix2 = torch.tensor([[5, 6], [7, 8]])\n",
    "print(\"Matrix multiplication:\", torch.mm(matrix1, matrix2))\n",
    "print(\"Matrix multiplication (similar):\", matrix1 @ matrix2)\n",
    "print(\"Hadamard product (element-wise multiplication):\", matrix1 * matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "View: tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "c = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "print(\"Reshape:\", c.reshape(2, 3))\n",
    "print(\"View:\", c.view(3, 2))  # View is similar to reshape but shares memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a Simple Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleNLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNLP, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Sequential layers\n",
    "        self.layer1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embed the input\n",
    "        # Output shape: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Average over sequence length (simple pooling)\n",
    "        # Output shape: (batch_size, embedding_dim)\n",
    "        pooled = torch.mean(embedded, dim=1)\n",
    "        \n",
    "        # Forward pass through layers\n",
    "        x = self.layer1(pooled)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.layer3(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 2  # e.g., binary classification\n",
    "batch_size = 32\n",
    "seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input data (simulating tokenized text)\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # Random token ids\n",
    "y = torch.randint(0, output_dim, (batch_size,))  # Random labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7542, 6067, 6876,  ..., 5294, 5693, 1677],\n",
      "        [5070, 7709, 2370,  ..., 4339, 3861, 9564],\n",
      "        [4270, 5553,  137,  ..., 1618, 9168,  407],\n",
      "        ...,\n",
      "        [7071, 9164, 7767,  ..., 6551, 1778,   38],\n",
      "        [3266, 9237, 8229,  ..., 2014, 7532, 3783],\n",
      "        [ 436, 9500, 7246,  ..., 7652, 4695, 1316]]) tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNLP(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()  # Common for classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(x)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(outputs, y)  # calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6897, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()  # calculate gradients (backpropagation)\n",
    "optimizer.step()  # update weights\n",
    "optimizer.zero_grad()  # clear gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(x)\n",
    "loss = criterion(outputs, y)  # calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6780, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss  # it's decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do it multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [00:00<00:00, 133.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 0.6780\n",
      "Epoch [2/40], Loss: 0.6669\n",
      "Epoch [3/40], Loss: 0.6554\n",
      "Epoch [4/40], Loss: 0.6424\n",
      "Epoch [5/40], Loss: 0.6276\n",
      "Epoch [6/40], Loss: 0.6101\n",
      "Epoch [7/40], Loss: 0.5897\n",
      "Epoch [8/40], Loss: 0.5661\n",
      "Epoch [9/40], Loss: 0.5389\n",
      "Epoch [10/40], Loss: 0.5083\n",
      "Epoch [11/40], Loss: 0.4742\n",
      "Epoch [12/40], Loss: 0.4369\n",
      "Epoch [13/40], Loss: 0.3969\n",
      "Epoch [14/40], Loss: 0.3551\n",
      "Epoch [15/40], Loss: 0.3122\n",
      "Epoch [16/40], Loss: 0.2693\n",
      "Epoch [17/40], Loss: 0.2276\n",
      "Epoch [18/40], Loss: 0.1883\n",
      "Epoch [19/40], Loss: 0.1523\n",
      "Epoch [20/40], Loss: 0.1204\n",
      "Epoch [21/40], Loss: 0.0930\n",
      "Epoch [22/40], Loss: 0.0702\n",
      "Epoch [23/40], Loss: 0.0520\n",
      "Epoch [24/40], Loss: 0.0379\n",
      "Epoch [25/40], Loss: 0.0272\n",
      "Epoch [26/40], Loss: 0.0194\n",
      "Epoch [27/40], Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 134.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/40], Loss: 0.0097\n",
      "Epoch [29/40], Loss: 0.0069\n",
      "Epoch [30/40], Loss: 0.0049\n",
      "Epoch [31/40], Loss: 0.0035\n",
      "Epoch [32/40], Loss: 0.0025\n",
      "Epoch [33/40], Loss: 0.0019\n",
      "Epoch [34/40], Loss: 0.0014\n",
      "Epoch [35/40], Loss: 0.0010\n",
      "Epoch [36/40], Loss: 0.0008\n",
      "Epoch [37/40], Loss: 0.0006\n",
      "Epoch [38/40], Loss: 0.0005\n",
      "Epoch [39/40], Loss: 0.0004\n",
      "Epoch [40/40], Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Forward pass\n",
    "    outputs = model(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example predictions for 2 sequences:\n",
      "tensor([[-2.7261,  2.7143],\n",
      "        [ 0.4964, -0.5536]])\n",
      "Predicted classes: tensor([1, 0])\n",
      "\n",
      "Model Architecture:\n",
      "SimpleNLP(\n",
      "  (embedding): Embedding(10000, 128)\n",
      "  (layer1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Input shape: torch.Size([32, 50])\n",
      "Output shape: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# So we don't calculate gradients which saves memory and computations\n",
    "with torch.no_grad():\n",
    "    # Sample input: (batch_size=2, seq_len=50)\n",
    "    test_input = torch.randint(0, vocab_size, (2, seq_len))\n",
    "    predictions = model(test_input)\n",
    "    print(\"\\nExample predictions for 2 sequences:\")\n",
    "    print(predictions)\n",
    "    print(\"Predicted classes:\", torch.argmax(predictions, dim=1))\n",
    "\n",
    "## Model architecture summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Shape summary\n",
    "print(\"\\nInput shape:\", x.shape)  # (batch_size, seq_len)\n",
    "print(\"Output shape:\", outputs.shape)  # (batch_size, output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sushi_biryani",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
