{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SpeculativeDecoding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        draft_model_name=\"distilgpt2\",\n",
    "        target_model_name=\"gpt2-medium\",\n",
    "        acceptance_threshold=0.9,\n",
    "    ):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load models and tokenizers\n",
    "        print(f\"Loading draft model: {draft_model_name}\")\n",
    "        self.draft_tokenizer = AutoTokenizer.from_pretrained(draft_model_name)\n",
    "        if self.draft_tokenizer.pad_token_id is None:\n",
    "            self.draft_tokenizer.pad_token = self.draft_tokenizer.eos_token\n",
    "        self.draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        print(f\"Loading target model: {target_model_name}\")\n",
    "        self.target_tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "        if self.target_tokenizer.pad_token_id is None:\n",
    "            self.target_tokenizer.pad_token = self.target_tokenizer.eos_token\n",
    "        self.target_model = AutoModelForCausalLM.from_pretrained(target_model_name).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        self.acceptance_threshold = acceptance_threshold\n",
    "\n",
    "    def generate(self, prompt, max_length=50, n_draft_tokens=4, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Generate text using speculative decoding.\n",
    "        \"\"\"\n",
    "        # Encode prompt\n",
    "        input_ids = self.draft_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\n",
    "            self.device\n",
    "        )\n",
    "        current_length = input_ids.shape[1]\n",
    "\n",
    "        # Generate until max length or EOS\n",
    "        while current_length < max_length:\n",
    "            # 1. Prepare padded sequence\n",
    "            # TODO: concatenate prompt with padding tokens\n",
    "            \n",
    "            # 2. Forward pass through draft model and get draft tokens by doing greedy decoding\n",
    "            with torch.no_grad():\n",
    "                # TODO: forward pass through draft model\n",
    "                # TODO: create draft_tokens by doing greedy decoding\n",
    "\n",
    "            # 3. Verify with target model\n",
    "            # use the target model to get the probabilities of the draft tokens\n",
    "            # then, calculate the acceptance probabilities by masking the draft tokens\n",
    "            # with the acceptance threshold\n",
    "            # TODO: implement this step\n",
    "\n",
    "            # Calculate acceptance probabilities\n",
    "            draft_token_probs = # TODO: calculate acceptance probabilities\n",
    "            \n",
    "            # get mask of accepted tokens\n",
    "            accepted_mask = draft_token_probs >= self.acceptance_threshold\n",
    "\n",
    "            # Find first rejection or accept all if no rejections\n",
    "            # then add accepted tokens to input (concatenate)\n",
    "            # TODO: implement this step\n",
    "\n",
    "            # Check for EOS token\n",
    "            if (input_ids == self.draft_tokenizer.eos_token_id).any():\n",
    "                break\n",
    "\n",
    "            # If no tokens were accepted, accept at least the first token \n",
    "            # from the draft model\n",
    "            if accept_length == 0:\n",
    "                accept_length = 1\n",
    "                input_ids = torch.cat([input_ids, draft_tokens[:, :1]], dim=1)\n",
    "                current_length = input_ids.shape[1]\n",
    "\n",
    "        # Decode final output\n",
    "        output_text = self.draft_tokenizer.decode(\n",
    "            input_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "        return output_text\n",
    "\n",
    "\n",
    "decoder = SpeculativeDecoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Compare greedy vs greedy with speculative decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
