{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Reward Model\n",
    "\n",
    "Alright, you know what preference data is, we will talk about Reward Model which will utilize this preference data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl.trainer.utils import RewardDataCollatorWithPadding\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you dive deep, let's observe what Reward Model is\n",
    "\n",
    "We will use \"OpenAssistant/reward-model-deberta-v3-base\", a model that has been trained using a preference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reward model and the tokenizerReward Model\n",
    "\n",
    "reward_name = \"OpenAssistant/reward-model-deberta-v3-base\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_name)\n",
    "reward_model = reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the outputs of the reward model\n",
    "question = \"I just came out of from jail, any suggestion of my future?\"\n",
    "helpful = \"It's great to hear that you have been released from jail.\"\n",
    "bad = \"Go to jail, lol!\"\n",
    "\n",
    "inputs = tokenizer(question, helpful, return_tensors='pt')\n",
    "good_score = reward_model(**inputs).logits[0].cpu().detach()\n",
    "\n",
    "inputs = tokenizer(question, bad, return_tensors='pt')\n",
    "bad_score = reward_model(**inputs).logits[0].cpu().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?\n",
    "Questions:\n",
    "1. How is the format of the input and output?\n",
    "2. How do you compare which one is prefered here?\n",
    "3. Play around with the input, what do you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know, let's dive into how to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your base model\n",
    "reward_name = \"google-bert/bert-base-uncased\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = RewardConfig(\n",
    "    output_dir=\"reward\",\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have trained our Reward Model, so we can train using PPO!\n",
    "\n",
    "You can try training it by using this (as per current version 14/01/2025):\n",
    "\n",
    "For instance:\n",
    "\n",
    "```sh\n",
    "accelerate launch --config_file examples/accelerate_configs/deepspeed_zero2.yaml \\\n",
    "    examples/scripts/ppo/ppo_tldr.py \\\n",
    "    --output_dir models/minimal/ppo_tldr \\\n",
    "    --learning_rate 3e-6 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --total_episodes 1000000 \\\n",
    "    --model_name_or_path EleutherAI/pythia-1b-deduped \\\n",
    "    --sft_model_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr \\\n",
    "    --reward_model_path CHANGE_TO_YOUR_MODEL_PATH \\\n",
    "    --local_rollout_forward_batch_size 16 \\\n",
    "    --missing_eos_penalty 1.0 \\\n",
    "    --stop_token eos\n",
    "```\n",
    "\n",
    "For more information:\n",
    "\n",
    "https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
    "\n",
    "We won't dive into PPO as this involves Reinforcement Learning (RL) method and refer you to above link.\n",
    "\n",
    "However, if you are familiar with RL, basically PPO in RL is what RLHF implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, to use huggingface, it's not difficult, isn't it? But, what's behind this `RewardTrainer`?\n",
    "\n",
    "Let's dive deep!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocess the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "        Preprocess the data to match TRL's expected format\n",
    "        TRl expects a dictionary with the following keys:\n",
    "        - input_ids_chosen\n",
    "        - attention_mask_chosen\n",
    "        - input_ids_rejected\n",
    "        - attention_mask_rejected\n",
    "    \"\"\"\n",
    "\n",
    "    chosen = tokenizer(examples['prompt'], examples[\"chosen\"], truncation=True)\n",
    "    rejected = tokenizer(examples['prompt'], examples[\"rejected\"], truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\", split=\"train\")\n",
    "\n",
    "# Preprocess dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    remove_columns=dataset.column_names,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Check whether it can be loaded or not.\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    collate_fn=RewardDataCollatorWithPadding(tokenizer),\n",
    "    batch_size=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(next(iter(data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Change the RewardTrainer\n",
    "\n",
    "Remember the objective function:\n",
    "\n",
    "\n",
    "$$L_{RM}(\\phi) = -\\frac{1}{|D|} \\sum_{(x,y^+,y^-) \\in D} \\log(\\sigma(r_\\phi(x,y^+) - r_\\phi(x,y^-)))$$\n",
    "\n",
    "Where:\n",
    "- $\\phi$ represents the reward model parameters\n",
    "- $D$ is the dataset of preference pairs\n",
    "- $x$ is the input prompt\n",
    "- $y^+$ is the preferred response\n",
    "- $y^-$ is the non-preferred response\n",
    "- $r_\\phi(x,y)$ is the reward score assigned by the model\n",
    "- $\\sigma$ is the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, **kwargs):\n",
    "        # Extract inputs for chosen and rejected\n",
    "        chosen_rewards = model(\n",
    "            input_ids=inputs[\"input_ids_chosen\"],\n",
    "            attention_mask=inputs[\"attention_mask_chosen\"]\n",
    "        ).logits\n",
    "\n",
    "        rejected_rewards = model(\n",
    "            input_ids=inputs[\"input_ids_rejected\"],\n",
    "            attention_mask=inputs[\"attention_mask_rejected\"]\n",
    "        ).logits\n",
    "\n",
    "        # Compute loss\n",
    "        # Standard preference loss without margin\n",
    "        loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./reward_model_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False  # Important\n",
    ")\n",
    "\n",
    "# Initialize trainer with TRL's collator\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Hello can I get your help?\"\n",
    "helpful = \"Sure, what can I do for you 😊?\"\n",
    "bad = \"As a research assistant, I don't want to help you!\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(question, helpful, return_tensors='pt').to('cuda')\n",
    "good_score = model(**inputs).logits[0].cpu().detach()\n",
    "\n",
    "inputs = tokenizer(question, bad, return_tensors='pt').to('cuda')\n",
    "bad_score = model(**inputs).logits[0].cpu().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
