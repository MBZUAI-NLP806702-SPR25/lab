{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/miniforge3/envs/sensei/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SpeculativeDecoding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        draft_model_name=\"distilgpt2\",  # change\n",
    "        target_model_name=\"gpt2\",  # change\n",
    "        acceptance_threshold=0.9,\n",
    "    ):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load models and tokenizers\n",
    "        print(f\"Loading draft model: {draft_model_name}\")\n",
    "        self.draft_tokenizer = AutoTokenizer.from_pretrained(draft_model_name)\n",
    "        if self.draft_tokenizer.pad_token_id is None:\n",
    "            self.draft_tokenizer.pad_token = self.draft_tokenizer.eos_token\n",
    "        self.draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        print(f\"Loading target model: {target_model_name}\")\n",
    "        self.target_tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "        if self.target_tokenizer.pad_token_id is None:\n",
    "            self.target_tokenizer.pad_token = self.target_tokenizer.eos_token\n",
    "        self.target_model = AutoModelForCausalLM.from_pretrained(target_model_name).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        self.acceptance_threshold = acceptance_threshold\n",
    "\n",
    "    def generate(self, prompt, max_length=50, n_draft_tokens=4, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Generate text using speculative decoding.\n",
    "        \"\"\"\n",
    "        # Encode prompt\n",
    "        input_ids = self.draft_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\n",
    "            self.device\n",
    "        )\n",
    "        current_length = input_ids.shape[1]\n",
    "\n",
    "        # Generate until max length or EOS\n",
    "        while current_length < max_length:\n",
    "            # 1. Generate draft tokens using the draft model\n",
    "            with torch.no_grad():                \n",
    "                # Generate n_draft_tokens autoregressively with the draft model\n",
    "                draft_tokens = []\n",
    "                draft_probs = []\n",
    "                current_draft_input = input_ids.clone()\n",
    "                \n",
    "                # Greedy decoding\n",
    "                for _ in range(n_draft_tokens):\n",
    "                    # Get probabilities for next token\n",
    "                    current_logits = self.draft_model(current_draft_input).logits[:, -1:]\n",
    "                    current_probs = F.softmax(current_logits / temperature, dim=-1)\n",
    "                    draft_probs.append(current_probs)\n",
    "                    \n",
    "                    # Sample or take argmax\n",
    "                    next_token = current_probs.argmax(dim=-1)  # You could also use sampling here\n",
    "                    draft_tokens.append(next_token)\n",
    "                    \n",
    "                    # Append to input for next iteration\n",
    "                    current_draft_input = torch.cat([current_draft_input, next_token], dim=1)\n",
    "                \n",
    "                # Stack all draft tokens\n",
    "                draft_tokens = torch.cat(draft_tokens, dim=1)  # Shape: (batch_size, n_draft_tokens)\n",
    "                draft_probs = torch.cat(draft_probs, dim=1)    # Shape: (batch_size, n_draft_tokens, vocab_size)\n",
    "\n",
    "            # 2. Verify with target model\n",
    "            proposed_sequence = torch.cat([input_ids, draft_tokens], dim=1)\n",
    "            with torch.no_grad():\n",
    "                target_outputs = self.target_model(proposed_sequence)\n",
    "                target_logits = target_outputs.logits[\n",
    "                    :, current_length - 1 : current_length + n_draft_tokens - 1\n",
    "                ]\n",
    "                target_probs = F.softmax(target_logits / temperature, dim=-1)\n",
    "\n",
    "            # Calculate acceptance probabilities\n",
    "            draft_token_probs = torch.gather(\n",
    "                target_probs, 2, draft_tokens.unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "            accepted_mask = draft_token_probs >= self.acceptance_threshold\n",
    "\n",
    "            # Find first rejection or accept all if no rejections\n",
    "            first_rejected = torch.where(~accepted_mask[0])[0]\n",
    "            accept_length = (\n",
    "                n_draft_tokens if len(first_rejected) == 0 else first_rejected[0].item()\n",
    "            )\n",
    "\n",
    "            if accept_length > 0:\n",
    "                # Add accepted tokens to input\n",
    "                input_ids = torch.cat(\n",
    "                    [input_ids, draft_tokens[:, :accept_length]], dim=1\n",
    "                )\n",
    "                current_length = input_ids.shape[1]\n",
    "\n",
    "            # Check for EOS token\n",
    "            if (input_ids == self.draft_tokenizer.eos_token_id).any():\n",
    "                break\n",
    "\n",
    "            # If no tokens were accepted, generate one token from the draft model\n",
    "            if accept_length == 0:\n",
    "                input_ids = torch.cat([input_ids, draft_tokens[:, :1]], dim=1)\n",
    "                current_length = input_ids.shape[1]\n",
    "\n",
    "        # Decode final output\n",
    "        output_text = self.draft_tokenizer.decode(\n",
    "            input_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading draft model: distilgpt2\n",
      "Loading target model: gpt2\n"
     ]
    }
   ],
   "source": [
    "decoder = SpeculativeDecoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.generate(\"Once upon a time,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
