{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will explain  how to use huggingface to do DPO!\n",
    "\n",
    "We will use `HumanLLMs/Human-Like-DPO-Dataset` using `datasets` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/miniforge3/envs/sensei/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl.trainer.dpo_trainer import PreferenceCollator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Just train it directly. We will use a base model `Qwen/Qwen2-0.5B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"Qwen2-0.5B-DPO\",\n",
    "    logging_steps=10,\n",
    "    max_length=64,\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "prompt = \"What's your favorite actor?\"\n",
    "\n",
    "output = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\"), max_new_tokens=10)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper understanding!\n",
    "\n",
    "Like the previous notebook, let's dive deep into the data and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"  # or your preferred model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Important for GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(x, tokenizer):\n",
    "    \"\"\"\n",
    "    preprocess_function is a function that takes in a single example and returns a dictionary of input_ids.\n",
    "    It follows the `PreferenceCollator` arguments in `trl` library.\n",
    "    \n",
    "    returned dictionary should have the following:\n",
    "    - prompt_input_ids: input_ids for the prompt\n",
    "    - chosen_input_ids: input_ids for the chosen option\n",
    "    - rejected_input_ids: input_ids for the rejected option\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        prompt_input_ids = tokenizer(x['prompt'], truncation=True, max_length=64)['input_ids'],\n",
    "        chosen_input_ids = tokenizer(x['chosen'], truncation=True, max_length=64)['input_ids'],\n",
    "        rejected_input_ids = tokenizer(x['rejected'], truncation=True, max_length=64)['input_ids']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer=tokenizer),\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it and see the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = PreferenceCollator(tokenizer.eos_token_id)\n",
    "dl = DataLoader(tokenized_train_dataset, collate_fn=collator, batch_size=2)\n",
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. Anything interesting with the data format? Yes, something is different, why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_length(\n",
    "    tensor: torch.Tensor, length: int, pad_value: Union[int, float], dim: int = -1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Helper function from `trl` library to pad a tensor to a certain length.\n",
    "    \n",
    "    Args:\n",
    "        tensor: input tensor\n",
    "        length: desired length\n",
    "        pad_value: value to pad with\n",
    "        dim: dimension to pad\n",
    "    \n",
    "    Returns:\n",
    "        padded tensor\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) >= length:\n",
    "        return tensor\n",
    "    else:\n",
    "        pad_size = list(tensor.shape)\n",
    "        pad_size[dim] = length - tensor.size(dim)\n",
    "        return torch.cat(\n",
    "            [\n",
    "                tensor,\n",
    "                pad_value\n",
    "                * torch.ones(*pad_size, dtype=tensor.dtype, device=tensor.device),\n",
    "            ],\n",
    "            dim=dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement these\n",
    "\n",
    "# DPO Loss Implementation Breakdown\n",
    "\n",
    "## 1. Core DPO Loss Formula\n",
    "The fundamental DPO loss being implemented is:\n",
    "$$L_{DPO}(\\theta) = -\\log(\\sigma(\\beta(r_\\theta(x,y^+) - r_\\theta(x,y^-)) - (r_{\\text{ref}}(x,y^+) - r_{\\text{ref}}(x,y^-))))$$\n",
    "\n",
    "## 2. Log Probability Calculation\n",
    "For each sequence, log probabilities are computed as:\n",
    "$$r_\\theta(x,y) = \\frac{1}{|y|}\\sum_{t=1}^{|y|} \\log P_\\theta(y_t|x,y_{<t})$$\n",
    "\n",
    "## 3. Policy and Reference Model Comparison\n",
    "The code computes four key components:\n",
    "1. Chosen policy logprobs: $r_\\theta(x,y^+)$\n",
    "2. Rejected policy logprobs: $r_\\theta(x,y^-)$\n",
    "3. Chosen reference logprobs: $r_{\\text{ref}}(x,y^+)$\n",
    "4. Rejected reference logprobs: $r_{\\text{ref}}(x,y^-)$\n",
    "\n",
    "Then calculates the ratios:\n",
    "$$\\text{policy\\_ratio} = r_\\theta(x,y^+) - r_\\theta(x,y^-)$$\n",
    "$$\\text{ref\\_ratio} = r_{\\text{ref}}(x,y^+) - r_{\\text{ref}}(x,y^-)$$\n",
    "\n",
    "## 4. Final Loss Computation\n",
    "\n",
    "$$L = -\\mathbb{E}[\\log\\sigma(\\beta((r_\\theta(x,y^+) - r_\\theta(x,y^-)) - (r_{\\text{ref}}(x,y^+) - r_{\\text{ref}}(x,y^-))))]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOTrainer(Trainer):\n",
    "    def __init__(self, processing_class, beta=0.1,  *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beta = beta\n",
    "        self.reference_model = deepcopy(self.model)\n",
    "        self.reference_model.eval()\n",
    "        self.processing_class = processing_class\n",
    "\n",
    "    def compute_loss(self, model, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Implementation follows trl's DPOTrainer `compute_loss` method (with a slight modification).\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate prompt and completion inputs (chosen and rejected)\n",
    "        # Repeat prompt inputs (and attention masks) for each completion\n",
    "        # for instance I have 2 completions, I will repeat the prompt inputs twice\n",
    "        # Then pad the completions to the same length (what dimension?)\n",
    "        prompt_input_ids = torch.cat(\n",
    "            (inputs[\"prompt_input_ids\"], inputs[\"prompt_input_ids\"]), dim=0\n",
    "        )\n",
    "        prompt_attention_mask = torch.cat(\n",
    "            (inputs[\"prompt_attention_mask\"], inputs[\"prompt_attention_mask\"]), dim=0\n",
    "        )\n",
    "\n",
    "        # pad them\n",
    "        max_completion_length = max(\n",
    "            inputs[\"chosen_input_ids\"].shape[1], inputs[\"rejected_input_ids\"].shape[1]\n",
    "        )\n",
    "\n",
    "        completion_input_ids = torch.cat(\n",
    "            (\n",
    "                pad_to_length(\n",
    "                    inputs[\"chosen_input_ids\"],\n",
    "                    max_completion_length,\n",
    "                    pad_value=self.tokenizer.pad_token_id,\n",
    "                ),\n",
    "                pad_to_length(\n",
    "                    inputs[\"rejected_input_ids\"],\n",
    "                    max_completion_length,\n",
    "                    pad_value=self.tokenizer.pad_token_id,\n",
    "                ),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        completion_attention_mask = torch.cat(\n",
    "            (\n",
    "                pad_to_length(\n",
    "                    inputs[\"chosen_attention_mask\"],\n",
    "                    max_completion_length,\n",
    "                    pad_value=self.tokenizer.pad_token_id,\n",
    "                ),\n",
    "                pad_to_length(\n",
    "                    inputs[\"rejected_attention_mask\"],\n",
    "                    max_completion_length,\n",
    "                    pad_value=self.tokenizer.pad_token_id,\n",
    "                ),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        \n",
    "        # Then concatenate the prompt and completions\n",
    "        input_ids = torch.cat((prompt_input_ids, completion_input_ids), dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            (prompt_attention_mask, completion_attention_mask), dim=1\n",
    "        )\n",
    "        \n",
    "        # Mask the prompt but not the completion for the loss\n",
    "        # illustration: if the input is [p,p,c,c,c], the loss mask will be [0,0,1,1,1]\n",
    "        loss_mask = torch.cat(\n",
    "            (torch.zeros_like(prompt_attention_mask), completion_attention_mask),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Memory optimization: Flush left to reduce memory usage\n",
    "        # illustration ( padding is 0), we remove the padding from the left\n",
    "        # We do this since we got two-sided padding\n",
    "        # input_ids = [\n",
    "        #     [0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        #     [0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        #     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "        #     [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "        # ]\n",
    "        for i in range(attention_mask.size(0)):\n",
    "            nonzero_indices = torch.nonzero(attention_mask[i])\n",
    "            if len(nonzero_indices) > 0:\n",
    "                first_one_idx = nonzero_indices[0].item()\n",
    "                input_ids[i] = torch.roll(input_ids[i], shifts=-first_one_idx)\n",
    "                attention_mask[i] = torch.roll(attention_mask[i], shifts=-first_one_idx)\n",
    "                loss_mask[i] = torch.roll(loss_mask[i], shifts=-first_one_idx)\n",
    "\n",
    "        # TODO: get model's output and its log probabilities\n",
    "        # Forward pass through policy model\n",
    "        policy_outputs = model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through reference model\n",
    "        with torch.no_grad():\n",
    "            reference_outputs = self.reference_model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "            )\n",
    "        batch_size = inputs[\"prompt_input_ids\"].shape[0]\n",
    "\n",
    "        policy_logps = self._get_logprobs(\n",
    "            policy_outputs.logits, input_ids, attention_mask, loss_mask\n",
    "        )\n",
    "\n",
    "        ref_logps = self._get_logprobs(\n",
    "            reference_outputs.logits, input_ids, attention_mask, loss_mask\n",
    "        )\n",
    "\n",
    "        # Split logprobs into chosen and rejected\n",
    "        chosen_policy_logps = policy_logps[:batch_size]\n",
    "        rejected_policy_logps = policy_logps[batch_size:]\n",
    "        chosen_ref_logps = ref_logps[:batch_size]\n",
    "        rejected_ref_logps = ref_logps[batch_size:]\n",
    "\n",
    "        # Compute policy and reference ratios\n",
    "        policy_ratio = chosen_policy_logps - rejected_policy_logps\n",
    "        ref_ratio = chosen_ref_logps - rejected_ref_logps\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = -torch.nn.functional.logsigmoid(\n",
    "            self.beta * (policy_ratio - ref_ratio)\n",
    "        ).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _get_logprobs(self, logits, input_ids, attention_mask, loss_mask):\n",
    "        \"\"\"Compute sequence-level log probabilities.\"\"\"\n",
    "        # Shift for next token prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        shift_mask = attention_mask[..., 1:].contiguous()\n",
    "        shift_loss_mask = loss_mask[..., 1:].contiguous()\n",
    "\n",
    "        # Get log probs\n",
    "        log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "        token_logprobs = torch.gather(\n",
    "            log_probs, dim=-1, index=shift_labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Only consider tokens that are part of the completion (not prompt)\n",
    "        masked_logprobs = token_logprobs * shift_mask * shift_loss_mask\n",
    "        sequence_logprobs = masked_logprobs.sum(dim=-1)\n",
    "\n",
    "        # Normalize by sequence length\n",
    "        sequence_lengths = (shift_mask * shift_loss_mask).sum(dim=-1).clamp(min=1e-5)\n",
    "        sequence_logprobs = sequence_logprobs / sequence_lengths\n",
    "\n",
    "        return sequence_logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dpo_trained_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    warmup_steps=100,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# 5. Initialize and run the DPO trainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    data_collator=collator,  # Use the PreferenceCollator\n",
    "    beta=0.1,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\"What's your favorite actor ?\"]\n",
    "out = model.generate(**tokenizer(input, return_tensors=\"pt\").to('cuda'), max_new_tokens=50)\n",
    "tokenizer.batch_decode(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = trainer.reference_model.generate(**tokenizer(input, return_tensors=\"pt\").to('cuda'), max_new_tokens=50)\n",
    "tokenizer.batch_decode(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
