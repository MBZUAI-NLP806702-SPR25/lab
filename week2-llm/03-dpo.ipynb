{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will explain  how to use huggingface to do DPO!\n",
    "\n",
    "We will use `HumanLLMs/Human-Like-DPO-Dataset` using `datasets` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/miniforge3/envs/sensei/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl.trainer.dpo_trainer import PreferenceCollator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Just train it directly. We will use a base model `Qwen/Qwen2-0.5B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"Qwen2-0.5B-DPO\",\n",
    "    logging_steps=10,\n",
    "    max_length=64,\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "prompt = \"What's your favorite actor?\"\n",
    "\n",
    "output = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\"), max_new_tokens=10)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper understanding!\n",
    "\n",
    "Like the previous notebook, let's dive deep into the data and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"  # or your preferred model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Important for GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(x, tokenizer):\n",
    "    \"\"\"\n",
    "    preprocess_function is a function that takes in a single example and returns a dictionary of input_ids.\n",
    "    It follows the `PreferenceCollator` arguments in `trl` library.\n",
    "    \n",
    "    returned dictionary should have the following:\n",
    "    - prompt_input_ids: input_ids for the prompt\n",
    "    - chosen_input_ids: input_ids for the chosen option\n",
    "    - rejected_input_ids: input_ids for the rejected option\n",
    "    \"\"\"\n",
    "    # TODO: implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: map and test it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it and see the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: map and test it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. Anything interesting with the data format? Yes, something is different, why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_length(\n",
    "    tensor: torch.Tensor, length: int, pad_value: Union[int, float], dim: int = -1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Helper function from `trl` library to pad a tensor to a certain length.\n",
    "    \n",
    "    Args:\n",
    "        tensor: input tensor\n",
    "        length: desired length\n",
    "        pad_value: value to pad with\n",
    "        dim: dimension to pad\n",
    "    \n",
    "    Returns:\n",
    "        padded tensor\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) >= length:\n",
    "        return tensor\n",
    "    else:\n",
    "        pad_size = list(tensor.shape)\n",
    "        pad_size[dim] = length - tensor.size(dim)\n",
    "        return torch.cat(\n",
    "            [\n",
    "                tensor,\n",
    "                pad_value\n",
    "                * torch.ones(*pad_size, dtype=tensor.dtype, device=tensor.device),\n",
    "            ],\n",
    "            dim=dim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement these\n",
    "\n",
    "# DPO Loss Implementation Breakdown\n",
    "\n",
    "## 1. Core DPO Loss Formula\n",
    "The fundamental DPO loss being implemented is:\n",
    "$$L_{DPO}(\\theta) = -\\log(\\sigma(\\beta(r_\\theta(x,y^+) - r_\\theta(x,y^-)) - (r_{\\text{ref}}(x,y^+) - r_{\\text{ref}}(x,y^-))))$$\n",
    "\n",
    "## 2. Log Probability Calculation\n",
    "For each sequence, log probabilities are computed as:\n",
    "$$r_\\theta(x,y) = \\frac{1}{|y|}\\sum_{t=1}^{|y|} \\log P_\\theta(y_t|x,y_{<t})$$\n",
    "\n",
    "## 3. Policy and Reference Model Comparison\n",
    "The code computes four key components:\n",
    "1. Chosen policy logprobs: $r_\\theta(x,y^+)$\n",
    "2. Rejected policy logprobs: $r_\\theta(x,y^-)$\n",
    "3. Chosen reference logprobs: $r_{\\text{ref}}(x,y^+)$\n",
    "4. Rejected reference logprobs: $r_{\\text{ref}}(x,y^-)$\n",
    "\n",
    "Then calculates the ratios:\n",
    "$$\\text{policy\\_ratio} = r_\\theta(x,y^+) - r_\\theta(x,y^-)$$\n",
    "$$\\text{ref\\_ratio} = r_{\\text{ref}}(x,y^+) - r_{\\text{ref}}(x,y^-)$$\n",
    "\n",
    "## 4. Final Loss Computation\n",
    "\n",
    "$$L = -\\mathbb{E}[\\log\\sigma(\\beta((r_\\theta(x,y^+) - r_\\theta(x,y^-)) - (r_{\\text{ref}}(x,y^+) - r_{\\text{ref}}(x,y^-))))]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOTrainer(Trainer):\n",
    "    def __init__(self, processing_class, beta=0.1,  *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beta = beta\n",
    "        self.processing_class = processing_class  # tokenizer\n",
    "\n",
    "    def compute_loss(self, model, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Implementation follows trl's DPOTrainer `compute_loss` method (with a slight modification).\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate prompt and completion inputs (chosen and rejected)\n",
    "        # Repeat prompt inputs (and attention masks) for each completion\n",
    "        # for instance I have 2 completions, I will repeat the prompt inputs twice\n",
    "        # Then pad the completions to the same length (what dimension?)\n",
    "        # TODO: implement them!\n",
    "        \n",
    "        \n",
    "        # Then concatenate the prompt and completions\n",
    "        # TODO: implement them!\n",
    "        \n",
    "        \n",
    "        # you should have input_ids and attention_mask now\n",
    "        # Mask the prompt but not the completion for the loss\n",
    "        # illustration: if the input is [p,p,c,c,c], the loss mask will be [0,0,1,1,1]\n",
    "        loss_mask = # TODO: implement this!\n",
    "\n",
    "        # Memory optimization: Flush left to reduce memory usage\n",
    "        # illustration ( padding is 0), we remove the padding from the left\n",
    "        # We do this since we got two-sided padding\n",
    "        # input_ids = [\n",
    "        #     [0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        #     [0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        #     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "        #     [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "        # ]\n",
    "        for i in range(attention_mask.size(0)):\n",
    "            nonzero_indices = torch.nonzero(attention_mask[i])\n",
    "            if len(nonzero_indices) > 0:\n",
    "                first_one_idx = nonzero_indices[0].item()\n",
    "                input_ids[i] = torch.roll(input_ids[i], shifts=-first_one_idx)\n",
    "                attention_mask[i] = torch.roll(attention_mask[i], shifts=-first_one_idx)\n",
    "                loss_mask[i] = torch.roll(loss_mask[i], shifts=-first_one_idx)\n",
    "\n",
    "        # TODO: get model's output and its log probabilities and the reference!\n",
    "        # Forward pass through policy model\n",
    "        \n",
    "        # Forward pass through reference model\n",
    "\n",
    "        batch_size = inputs[\"prompt_input_ids\"].shape[0]\n",
    "\n",
    "        # Get log probabilities\n",
    "\n",
    "        # Split logprobs into chosen and rejected\n",
    "        chosen_policy_logps = policy_logps[:batch_size]\n",
    "        rejected_policy_logps = policy_logps[batch_size:]\n",
    "        chosen_ref_logps = ref_logps[:batch_size]\n",
    "        rejected_ref_logps = ref_logps[batch_size:]\n",
    "\n",
    "        # Compute policy and reference ratios\n",
    "        policy_ratio = # TODO: implement this!\n",
    "        ref_ratio = # TODO: implement this!\n",
    "\n",
    "        # Compute the loss\n",
    "        loss =  # TODO: implement this!\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _get_logprobs(self, logits, input_ids, attention_mask, loss_mask):\n",
    "        \"\"\"Compute sequence-level log probabilities.\n",
    "        Args:\n",
    "            logits: Logits from the model. Shape: (batch_size, seq_len, vocab_size)\n",
    "            input_ids: Input IDs. Shape: (batch_size, seq_len)\n",
    "            attention_mask: Attention mask. Shape: (batch_size, seq_len)\n",
    "            loss_mask: Loss mask for the prompt. Shape: (batch_size, seq_len)\n",
    "        Returns:\n",
    "            sequence_logprobs: Sequence-level log probabilities. Shape: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Shift for next token prediction\n",
    "\n",
    "\n",
    "        # Get log probs\n",
    "\n",
    "\n",
    "        # Only consider tokens that are part of the completion (not prompt)\n",
    "        # hint: use mask\n",
    "\n",
    "        # Normalize by sequence length\n",
    "\n",
    "\n",
    "        return sequence_logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dpo_trained_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    warmup_steps=100,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# 5. Initialize and run the DPO trainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    data_collator=collator,  # Use the PreferenceCollator\n",
    "    beta=0.1,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\"What's your favorite actor ?\"]\n",
    "\n",
    "# TODO: test the model and compare with the reference model (use model and trainer.reference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with the reference model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
